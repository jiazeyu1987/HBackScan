#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒæ”¶æµ‹è¯•è¿è¡Œè„šæœ¬
æ‰§è¡Œå®Œæ•´çš„éªŒæ”¶æµ‹è¯•å¥—ä»¶å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import os
import sys
import time
import shutil
import argparse
import subprocess
from pathlib import Path
from datetime import datetime


class AcceptanceTestRunner:
    """éªŒæ”¶æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, test_dir="tests", report_dir="reports/acceptance"):
        self.test_dir = Path(test_dir)
        self.report_dir = Path(report_dir)
        self.start_time = None
        self.end_time = None
        
    def setup_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¡®ä¿æµ‹è¯•æ•°æ®ç›®å½•å­˜åœ¨
        (self.report_dir / "data").mkdir(exist_ok=True)
        
        print(f"âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆï¼ŒæŠ¥å‘Šç›®å½•: {self.report_dir}")
    
    def run_pytest(self, args=None, description=""):
        """è¿è¡Œpytestæµ‹è¯•"""
        if args is None:
            args = []
            
        cmd = ["pytest"] + args
        
        print(f"ğŸš€ {description}")
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print("-" * 50)
        
        start_time = time.time()
        
        try:
            result = subprocess.run(
                cmd,
                cwd=Path.cwd(),
                capture_output=False,  # æ˜¾ç¤ºå®æ—¶è¾“å‡º
                text=True
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"âœ… {description} å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return result.returncode == 0, duration
            
        except Exception as e:
            print(f"âŒ {description} å¤±è´¥: {e}")
            return False, 0
    
    def run_fast_acceptance_tests(self):
        """è¿è¡Œå¿«é€ŸéªŒæ”¶æµ‹è¯•"""
        args = [
            str(self.test_dir),
            "-v",
            "-m", "acceptance and fast",
            "--html", str(self.report_dir / "fast_acceptance_report.html"),
            "--self-contained-html",
            "--tb=short",
            "--timeout=60"
        ]
        
        return self.run_pytest(args, "å¿«é€ŸéªŒæ”¶æµ‹è¯•")
    
    def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½éªŒæ”¶æµ‹è¯•"""
        args = [
            str(self.test_dir),
            "-v", 
            "-m", "acceptance and performance",
            "--html", str(self.report_dir / "performance_acceptance_report.html"),
            "--self-contained-html",
            "--tb=short",
            "--timeout=180"
        ]
        
        return self.run_pytest(args, "æ€§èƒ½éªŒæ”¶æµ‹è¯•")
    
    def run_workflow_tests(self):
        """è¿è¡Œå·¥ä½œæµéªŒæ”¶æµ‹è¯•"""
        args = [
            str(self.test_dir / "test_acceptance.py::TestAcceptanceScenarios::test_complete_refresh_workflow"),
            "-v",
            "--html", str(self.report_dir / "workflow_acceptance_report.html"),
            "--self-contained-html",
            "--tb=long",
            "--timeout=300"
        ]
        
        return self.run_pytest(args, "å®Œæ•´å·¥ä½œæµéªŒæ”¶æµ‹è¯•")
    
    def run_complete_acceptance_suite(self):
        """è¿è¡Œå®Œæ•´éªŒæ”¶æµ‹è¯•å¥—ä»¶"""
        args = [
            str(self.test_dir),
            "-v",
            "-m", "acceptance",
            "--html", str(self.report_dir / "complete_acceptance_report.html"),
            "--self-contained-html",
            "--junit-xml", str(self.report_dir / "junit.xml"),
            "--cov=.",
            "--cov-report=html:" + str(self.report_dir / "coverage"),
            "--cov-report=term-missing",
            "--cov-report=xml:" + str(self.report_dir / "coverage.xml"),
            "--cov-fail-under=75",
            "--tb=short",
            "--timeout=300"
        ]
        
        return self.run_pytest(args, "å®Œæ•´éªŒæ”¶æµ‹è¯•å¥—ä»¶")
    
    def generate_summary_report(self, results):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        summary_file = self.report_dir / "acceptance_summary.md"
        
        total_duration = self.end_time - self.start_time if self.start_time and self.end_time else 0
        
        summary_content = f"""# éªŒæ”¶æµ‹è¯•æ‰§è¡ŒæŠ¥å‘Š

**æ‰§è¡Œæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ€»è€—æ—¶**: {total_duration:.2f}ç§’

## æµ‹è¯•ç»“æœæ¦‚è§ˆ

"""
        
        for test_name, (success, duration) in results.items():
            status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
            summary_content += f"- **{test_name}**: {status} (è€—æ—¶: {duration:.2f}ç§’)\n"
        
        passed_count = sum(1 for _, (success, _) in results.items() if success)
        total_count = len(results)
        success_rate = (passed_count / total_count * 100) if total_count > 0 else 0
        
        summary_content += f"""
## æµ‹è¯•ç»Ÿè®¡

- **æ€»æµ‹è¯•æ•°**: {total_count}
- **é€šè¿‡æ•°**: {passed_count}
- **å¤±è´¥æ•°**: {total_count - passed_count}
- **æˆåŠŸç‡**: {success_rate:.1f}%

## ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶

- å¿«é€ŸéªŒæ”¶æµ‹è¯•: `fast_acceptance_report.html`
- æ€§èƒ½éªŒæ”¶æµ‹è¯•: `performance_acceptance_report.html`
- å·¥ä½œæµéªŒæ”¶æµ‹è¯•: `workflow_acceptance_report.html`
- å®Œæ•´éªŒæ”¶æµ‹è¯•: `complete_acceptance_report.html`
- è¦†ç›–ç‡æŠ¥å‘Š: `coverage/index.html`
- JUnitæŠ¥å‘Š: `junit.xml`

## è¦†ç›–ç‡è¦æ±‚

- æœ€ä½è¦†ç›–ç‡: 75%
- å®é™…è¦†ç›–ç‡: æŸ¥çœ‹è¦†ç›–ç‡æŠ¥å‘Š

## æ€§èƒ½è¦æ±‚

- APIå“åº”æ—¶é—´: < 500ms
- æµ‹è¯•å®Œæˆæ—¶é—´: < 5åˆ†é’Ÿï¼ˆä¸»è¦å·¥ä½œæµï¼‰

## éªŒæ”¶æ ‡å‡†

- [ ] å®Œæ•´åˆ·æ–°å·¥ä½œæµæµ‹è¯•é€šè¿‡
- [ ] APIæ€§èƒ½è¦æ±‚æ»¡è¶³
- [ ] é‡å¤åˆ·æ–°é˜²æŠ¤ç”Ÿæ•ˆ
- [ ] ç©ºæ•°æ®åº“åˆå§‹åŒ–æ­£å¸¸
- [ ] éƒ¨åˆ†æ•°æ®ç¼ºå¤±å¤„ç†æ­£ç¡®
- [ ] ç½‘ç»œè¶…æ—¶å’Œé”™è¯¯æ¢å¤æ­£å¸¸
- [ ] è¾¹ç•Œæ¡ä»¶å¤„ç†æ­£ç¡®
- [ ] æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡

"""
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        print(f"ğŸ“„ æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_file}")
        return summary_file
    
    def copy_logs(self):
        """å¤åˆ¶æ—¥å¿—æ–‡ä»¶"""
        logs_source = Path("logs")
        logs_dest = self.report_dir / "logs"
        
        if logs_source.exists():
            shutil.copytree(logs_source, logs_dest, dirs_exist_ok=True)
            print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶å·²å¤åˆ¶åˆ°: {logs_dest}")
    
    def run(self, test_type="all"):
        """è¿è¡ŒéªŒæ”¶æµ‹è¯•"""
        self.start_time = time.time()
        
        print("ğŸ¯ å¼€å§‹æ‰§è¡ŒéªŒæ”¶æµ‹è¯•å¥—ä»¶")
        print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        # è®¾ç½®ç¯å¢ƒ
        self.setup_environment()
        
        results = {}
        
        # æ ¹æ®æµ‹è¯•ç±»å‹è¿è¡Œç›¸åº”çš„æµ‹è¯•
        if test_type == "fast" or test_type == "all":
            success, duration = self.run_fast_acceptance_tests()
            results["å¿«é€ŸéªŒæ”¶æµ‹è¯•"] = (success, duration)
            
            if not success and test_type == "fast":
                print("âŒ å¿«é€ŸéªŒæ”¶æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                return False
            
            print()
        
        if test_type == "performance" or test_type == "all":
            success, duration = self.run_performance_tests()
            results["æ€§èƒ½éªŒæ”¶æµ‹è¯•"] = (success, duration)
            print()
        
        if test_type == "workflow" or test_type == "all":
            success, duration = self.run_workflow_tests()
            results["å·¥ä½œæµéªŒæ”¶æµ‹è¯•"] = (success, duration)
            print()
        
        if test_type == "complete" or test_type == "all":
            success, duration = self.run_complete_acceptance_suite()
            results["å®Œæ•´éªŒæ”¶æµ‹è¯•å¥—ä»¶"] = (success, duration)
            print()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.end_time = time.time()
        summary_file = self.generate_summary_report(results)
        self.copy_logs()
        
        # æ€»ç»“ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“Š éªŒæ”¶æµ‹è¯•æ‰§è¡Œå®Œæˆ")
        
        passed_count = sum(1 for _, (success, _) in results.items() if success)
        total_count = len(results)
        success_rate = (passed_count / total_count * 100) if total_count > 0 else 0
        
        print(f"âœ… é€šè¿‡: {passed_count}/{total_count} ({success_rate:.1f}%)")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {self.report_dir}")
        print(f"ğŸ“„ æ‘˜è¦æŠ¥å‘Š: {summary_file}")
        
        if success_rate >= 80:  # 80%é€šè¿‡ç‡è®¤ä¸ºé€šè¿‡
            print("ğŸ‰ éªŒæ”¶æµ‹è¯•æ€»ä½“é€šè¿‡!")
            return True
        else:
            print("âŒ éªŒæ”¶æµ‹è¯•æœªè¾¾åˆ°é€šè¿‡æ ‡å‡†")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡ŒéªŒæ”¶æµ‹è¯•å¥—ä»¶")
    parser.add_argument(
        "--type", 
        choices=["fast", "performance", "workflow", "complete", "all"],
        default="all",
        help="æµ‹è¯•ç±»å‹"
    )
    parser.add_argument(
        "--test-dir",
        default="tests",
        help="æµ‹è¯•ç›®å½•"
    )
    parser.add_argument(
        "--report-dir", 
        default="reports/acceptance",
        help="æŠ¥å‘Šç›®å½•"
    )
    
    args = parser.parse_args()
    
    runner = AcceptanceTestRunner(args.test_dir, args.report_dir)
    success = runner.run(args.type)
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()